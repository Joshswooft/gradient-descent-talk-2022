export default function gradient_descent() {
    // given a hypothesis theta_0, theta_1
    // optimise the cost function in this case MSE

    theta_0 = 0;
    theta_1 = 1;
    learning_rate = 0.1;
    let err = 10000000;
    // repeat until convergence
    while (err > 10) {
        t0 = theta_0 - learning_rate * 
    }
}